[
  {
    "paperId": "7b489c47ebca2c1df0e611e76b9649f69e4fce29",
    "paperLink": "https://www.semanticscholar.org/paper/7b489c47ebca2c1df0e611e76b9649f69e4fce29",
    "paperTitle": "Automatically Generated Summaries of Video Lectures May Enhance Students’ Learning Experience",
    "candidates": [
      {
        "rank": 1,
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners",
        "contexts": "(3) Fine-tuning GPT-3 . | If so, GPT-3 would be prompted to generate a new output on the same prompt. | In the third approach, we experimented with fine-tuning GPT-3 to perform lecture summarization. | When fine-tuning a GPT-3 model, we no longer need to provide prompts like we did in the previous two approaches. | We use the ‘text-davinci-002’ version of GPT-3 for our zero-shot and few-shot experiments, and as the basis for our fine-tuned davinci model. | We are encouraged by recent research in natural language processing demonstrating that people often prefer GPT-3 generated summaries over other methods of automatically generated summaries for news (Goyal et al., 2022). | This is the maximum length that can be encoded by GPT-3. | It is, also, unclear for which tasks it is sufficient to give prompts to achieve successful GPT-3 responses or whether there is a need for fine-tuning a model. | It is well-known, albeit not well-understood, that the quality of GPT-3 language generation is affected by the given prompt even if the differences in the way the prompt is articulated are not semantically very different. | In this work, we investigate the feasibility of automatically summarizing lecture videos’ transcripts using recent advances in large language models such as GPT-3 (Brown et al., 2020). | In this approach, we provide a few training examples to the GPT-3 model and we call it the \"few shot prompt\" condition. | …have been especially successful in generating fluent responses in a variety of natural language generation tasks with zero or few-shot prompting (Brown et al., 2020; Liu et al., 2023), instruction following (Sanh et al., 2021; Mishra et al., 2022; Bach et al., 2022), or fine-tuning (Howard and…",
        "reason": "GPT-3 is the operational backbone of the method (zero-shot, few-shot, and fine-tuned summarization). The system and study directly rely on GPT-3; without this model family the core approach would not exist.",
        "category": "High"
      },
      {
        "rank": 2,
        "paperId": "1bcd56b140a869fa8489cc6ed7e9088d13b2ffdd",
        "title": "Automatic Summarization of Lecture Slides for Enhanced Student Preview–Technical Report and User Study–",
        "contexts": "For example, Shimada et al. (2017) find * These authors contributed equally. that students using summaries of slides for preview have higher pre-quiz scores and spend less time, compared to students previewing original learning materials.",
        "reason": "Provides closely related empirical evidence that study materials with summaries improve learning outcomes, informing the framing and motivation of the user study and expected effects.",
        "category": "Medium"
      },
      {
        "rank": 3,
        "paperId": "83851f1a32d41975582ca62355858ab5e34738f7",
        "title": "News Summarization and Evaluation in the Era of GPT-3",
        "contexts": "We are encouraged by recent research in natural language processing demonstrating that people often prefer GPT-3 generated summaries over other methods of automatically generated summaries for news (Goyal et al., 2022).",
        "reason": "Supports the choice of GPT-3 for summarization by showing strong prior results in another domain (news), helping motivate feasibility and quality but not indispensable to the method.",
        "category": "Medium"
      },
      {
        "rank": 4,
        "paperId": "3db14551d878d66d1b17c77ee717e2a71ebd387c",
        "title": "Video digests: a browsable, skimmable format for informational lecture videos",
        "contexts": "Others have created video digests that are organized into a textbook-like format with chapters, titles, and sections with text summaries (Pavel et al., 2014). | Pavel et al. (2014)’s system provides an authoring interface that allows video authors to manually write textual summaries of a video themselves or to send the video to a crowdsourcing service to have summaries written.",
        "reason": "Domain-relevant prior on textual summaries for lecture videos and user-facing presentation, helping situate the contribution among video summarization approaches.",
        "category": "Medium"
      },
      {
        "rank": 5,
        "paperId": "ea81690fe44b6689afe3bc13729a2e74a50cece5",
        "title": "CatchLive: Real-time Summarization of Live Streams with Stream Content and Interaction Data",
        "contexts": "Past research in human-computer interaction aimed to improve educational videos via interactive transcripts, word clouds, keyword search, and highlight storyboards (Kim et al., 2014), or by segmenting the videos to present highlight moments with snap-shots and transcripts (Yang et al., 2022). | , 2014), or by segmenting the videos to present highlight moments with snapshots and transcripts (Yang et al., 2022).",
        "reason": "Related video summarization work indicating alternative ways to summarize or highlight video content; helpful context but not central to the method or study.",
        "category": "Medium"
      },
      {
        "rank": 6,
        "paperId": "20d874c0f7859e3278ef7b94f8645929e9795593",
        "title": "Data-driven interaction techniques for improving navigation of educational videos",
        "contexts": "Past research in human-computer interaction aimed to improve educational videos via interactive transcripts, word clouds, keyword search, and highlight storyboards (Kim et al., 2014), or by segmenting the videos to present highlight moments with snap-shots and transcripts (Yang et al., 2022).",
        "reason": "Background on improving educational video navigation; useful for positioning but not directly used in the approach.",
        "category": "Low"
      },
      {
        "rank": 7,
        "paperId": "640e362093fa57cce35f8f71e8f5d83c43ba138f",
        "title": "Self-regulated learning support in flipped learning videos enhances learning outcomes",
        "contexts": "Video lectures have been an important part of scaled online courses and flipped classrooms for several years, and have become widely used for an increasingly larger range of courses as a sub-stitute for students unable to attend class due to the COVID-19 pandemic (van Alten et al., 2020).",
        "reason": "Provides general context about the prevalence and importance of video lectures; peripheral to the core contribution.",
        "category": "Low"
      },
      {
        "rank": 8,
        "paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
        "title": "Finetuned Language Models Are Zero-Shot Learners",
        "contexts": "…in generating fluent responses in a variety of natural language generation tasks with zero or few-shot prompting (Brown et al., 2020; Liu et al., 2023), instruction following (Sanh et al., 2021; Mishra et al., 2022; Bach et al., 2022), or fine-tuning (Howard and Ruder, 2018; Wei et al., 2021).",
        "reason": "Generic LLM fine-tuning background; cited to situate LLM capabilities but not used directly.",
        "category": "Low"
      },
      {
        "rank": 9,
        "paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca",
        "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
        "contexts": "…in generating fluent responses in a variety of natural language generation tasks with zero or few-shot prompting (Brown et al., 2020; Liu et al., 2023), instruction following (Sanh et al., 2021; Mishra et al., 2022; Bach et al., 2022), or fine-tuning (Howard and Ruder, 2018; Wei et al., 2021).",
        "reason": "Background on instruction-following models; not directly applied in the method.",
        "category": "Low"
      },
      {
        "rank": 10,
        "paperId": "cbdb45fc16b0885905b91d84281c310e6cb49e9c",
        "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions",
        "contexts": "…in generating fluent responses in a variety of natural language generation tasks with zero or few-shot prompting (Brown et al., 2020; Liu et al., 2023), instruction following (Sanh et al., 2021; Mishra et al., 2022; Bach et al., 2022), or fine-tuning (Howard and Ruder, 2018; Wei et al., 2021).",
        "reason": "Cited as part of a general survey of instruction-following LLMs; peripheral to the contribution.",
        "category": "Low"
      },
      {
        "rank": 11,
        "paperId": "9b1f4492a663c7f56f2b43ae1ed167d3857aacca",
        "title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts",
        "contexts": "…in generating fluent responses in a variety of natural language generation tasks with zero or few-shot prompting (Brown et al., 2020; Liu et al., 2023), instruction following (Sanh et al., 2021; Mishra et al., 2022; Bach et al., 2022), or fine-tuning (Howard and Ruder, 2018; Wei et al., 2021).",
        "reason": "Prompt engineering resource cited for general background; not used in the pipeline.",
        "category": "Low"
      },
      {
        "rank": 12,
        "paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a",
        "title": "Universal Language Model Fine-tuning for Text Classification",
        "contexts": "…in generating fluent responses in a variety of natural language generation tasks with zero or few-shot prompting (Brown et al., 2020; Liu et al., 2023), instruction following (Sanh et al., 2021; Mishra et al., 2022; Bach et al., 2022), or fine-tuning (Howard and Ruder, 2018; Wei et al., 2021).",
        "reason": "Classic fine-tuning reference included for completeness; unrelated to the specific GPT-3 summarization approach used.",
        "category": "Low"
      }
    ]
  }
]