[
  {
    "paperId": "19f59c14b3d79e3203c696128a135d33eb35e468",
    "paperLink": "https://www.semanticscholar.org/paper/19f59c14b3d79e3203c696128a135d33eb35e468",
    "paperTitle": "Pragmatic Inference with a CLIP Listener for Contrastive Captioning",
    "candidates": [
      {
        "rank": 1,
        "paperId": "92dbc1509b5641946cc8b524610cb6803d6ee5f6",
        "title": "Reasoning about Pragmatics with Neural Listeners and Speakers",
        "contexts": "Prior work on contrastive/discriminative captioning investigates the tradeoff of model performance between discriminativeness and accuracy/fluency (Wang et al., 2021; Liu et al., 2019; Honda et al., 2022; Cho et al., 2022; Vedantam et al., 2017; Andreas and Klein, 2016). | Past work on discriminative captioning has successfully applied techniques from computational pragmatics to trade off between the two criteria above (Andreas and Klein, 2016; Vedantam et al., 2017; Cohn-Gordon et al., 2018). | , 2020) or (2) using a separate discriminative model but selecting a discriminative caption from among a set of entire captions generated by the speaker model for the target image (Andreas and Klein, 2016; Luo and Shakhnarovich, 2017). | …(Vedantam et al., 2017; Cohn-Gordon et al., 2018; Nie et al., 2020) or (2) using a separate discriminative model but selecting a discriminative caption from among a set of entire captions generated by the speaker model for the target image (Andreas and Klein, 2016; Luo and Shakhnarovich, 2017). | Thus, we adopt a sub-sampling approach similar to Andreas and Klein (2016); Fried et al. (2018).",
        "reason": "Core conceptual backbone for pragmatic speaker-listener inference that the paper directly builds upon and contrasts with by introducing a CLIP-based listener.",
        "category": "High"
      },
      {
        "rank": 2,
        "paperId": "945e97dcab001986d233a61a233fc524543182ad",
        "title": "Pragmatic Language Interpretation as Probabilistic Inference",
        "contexts": "Pragmatics Our approach to contrastive generation follows a long line of work on computational pragmatics, particularly in the Rational Speech Acts framework (Frank and Goodman, 2012; Goodman and Frank, 2016) which models language generation as an interaction between speakers and listeners.",
        "reason": "Foundational RSA framework that underpins the paper’s pragmatic inference formulation of speaker-listener interaction.",
        "category": "High"
      },
      {
        "rank": 3,
        "paperId": "ed5483d0669ae3f7146d432119f6540e461914e8",
        "title": "Image Retrieval from Contextual Descriptions",
        "contexts": "We posit the performance mismatch on human written captions is because it is challenging for neural retrieval models like ALBEF to interpret human-written descriptions, which are highly nuanced and grammatically complex (Krojer et al., 2022). | We posit that the performance mismatch on human written captions is because it is challenging for neural retrieval models like ALBEF to interpret human-written descriptions, which are highly nuanced and grammatically complex (Krojer et al., 2022). | We evaluate PICL on ImageCoDe (Krojer et al., 2022), a dataset originally designed for image retrieval with contextual descriptions. | To evaluate PICL, we conduct experiments with sets of images from ImageCoDe (Krojer et al., 2022), a challenging dataset originally designed for contrastive retrieval: retrieving target images among a set of distractors given contextual descriptions. | 4 As shown in Table 1, finetuned ALBEF outperforms the best-performing retrieval model from previous work (Krojer et al., 2022) on ImageCoDe with human-written captions, so we use | We use sets of images collected in ImageCoDe (Krojer et al., 2022) to evaluate the proposed approach. | 4 As shown in Table 1, finetuned ALBEF outperforms the best-performing retrieval model from previous work (Krojer et al., 2022) on Im-ageCoDe with human-written captions, so we use ALBEF-finetuned as our evaluating listener in automatic evaluations of informativeness.",
        "reason": "Primary evaluation setting (ImageCoDe) used to test informativeness with distractors; central to experimental design and claims.",
        "category": "High"
      },
      {
        "rank": 4,
        "paperId": "b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1",
        "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation",
        "contexts": ", 2021); we additionally use a separate architecture for the evaluative listener, dissimilar from our CLIP listener: the ALBEF vision-language model (Li et al., 2021).",
        "reason": "ALBEF serves as the evaluating listener for automatic informativeness, directly enabling the paper’s key evaluation protocol.",
        "category": "Medium"
      },
      {
        "rank": 5,
        "paperId": "ecce44df1956db4ec486539c6543345344809958",
        "title": "Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework",
        "contexts": "Finetuned OFA is a strong base captioner; at the time of this work, it achieves state-of-the-art performance on MSCOCO Image Captioning. | In particular, we use OFA (Wang et al., 2022), a unified sequence-to-sequence multimodal pretraining model and finetune it on MSCOCO Image Captioning dataset (Chen et al., 2015). | In particular, we use OFA3 (Wang et al., 2022), a unified sequence-to-sequence multimodal pretraining model and finetune it on MSCOCO Image Captioning dataset (Chen et al.",
        "reason": "Provides the base captioner (speaker) used in the pragmatic framework; important operational component though replaceable by other captioners.",
        "category": "Medium"
      },
      {
        "rank": 6,
        "paperId": "696ca58d93f6404fea0fc75c62d1d7b378f47628",
        "title": "Microsoft COCO Captions: Data Collection and Evaluation Server",
        "contexts": ", 2022), a unified sequence-to-sequence multimodal pretraining model and finetune it on MSCOCO Image Captioning dataset (Chen et al., 2015). | In particular, we use OFA (Wang et al., 2022), a unified sequence-to-sequence multimodal pretraining model and finetune it on MSCOCO Image Captioning dataset (Chen et al., 2015).",
        "reason": "Training data for the base captioner; operationally necessary for their specific setup but not unique to the method.",
        "category": "Medium"
      },
      {
        "rank": 7,
        "paperId": "673c1512aeb604c9070605ef097d6dfd5e4cd0ba",
        "title": "Communication-based Evaluation for Natural Language Generation",
        "contexts": "Informativeness Following Cohn-Gordon et al. (2018) and Newman et al. (2020), we evaluate the informativeness of captions generated by our method and baselines using a listener test : whether an evaluative listener model could identify the target image out of the distractors, given generated… | For informativeness, we follow previous work (Cohn-Gordon et al., 2018; Newman et al., 2020) to automatically evaluate the performance of pragmatic models with an evaluating listener L eval . | For informativeness, we follow previous work (Cohn-Gordon et al., 2018; Newman et al., 2020) to automatically evaluate the performance of pragmatic models with an evaluating lis-",
        "reason": "Establishes the listener-based informativeness evaluation protocol that the paper adopts.",
        "category": "Medium"
      },
      {
        "rank": 8,
        "paperId": "025f852b227766c3a5dc914ded6f6c0ae137c617",
        "title": "Pragmatically Informative Image Captioning with Character-Level Inference",
        "contexts": "Informativeness Following Cohn-Gordon et al. (2018) and Newman et al. (2020), we evaluate the informativeness of captions generated by our method and baselines using a listener test : whether an evaluative listener model could identify the target image out of the distractors, given generated… | For informativeness, we follow previous work (Cohn-Gordon et al., 2018; Newman et al., 2020) to automatically evaluate the performance of pragmatic models with an evaluating listener L eval . | Our approach builds on a family of inferencetime pragmatic-based contrastive captioning methods which have taken one of two approaches: (1) incrementally generating captions but using only a captioning model (our speaker model), where tokens are chosen that have high probability for the target image and low probability for the distractor (Vedantam et al., 2017; Cohn-Gordon et al., 2018; Nie et al., 2020) or (2) using a separate discriminative model but selecting a discriminative caption from among a set of entire captions generated by the speaker model for the target image (Andreas and Klein, 2016; Luo and Shakhnarovich, 2017). | Past work on discriminative captioning has successfully applied techniques from computational pragmatics to trade off between the two criteria above (Andreas and Klein, 2016; Vedantam et al., 2017; Cohn-Gordon et al., 2018). | For informativeness, we follow previous work (Cohn-Gordon et al., 2018; Newman et al., 2020) to automatically evaluate the performance of pragmatic models with an evaluating lis- | …model), where tokens are chosen that have high probability for the target image and low probability for the distractor (Vedantam et al., 2017; Cohn-Gordon et al., 2018; Nie et al., 2020) or (2) using a separate discriminative model but selecting a discriminative caption from among a set of… | Specifically, we derive the Bayesian RSA model introduced in Cohn-Gordon et al. (2018) from our base speaker S 0 , which enables direct comparison with our proposed approach. | Incre-RSA We further implement the incremental RSA model (Incre-RSA) from Cohn-Gordon et al. (2018) as a competitive baseline. | To mitigate this codebooking issue in evaluation, past work has made the evaluative listener dissimilar by training it on separate data (Cohn-Gordon et al., 2018; Kim et al., 2019; Fried et al., 2021); we additionally use a separate architecture for the evaluative listener, dissimilar from our CLIP… | Informativeness Following Cohn-Gordon et al. (2018) and Newman et al. (2020), we evaluate the informativeness of captions generated by our method and baselines using a listener test : whether an evaluative listener model could identify the target image out of the distractors, given generated captions. | Similar to Cohn-Gordon et al. (2018), we formulate the process of generating contrastive cap-tions as a series of reference games between two agents, a speaker and a listener . | These past works typically obtain the listener scores from the image captioning (speaker) model itself, for example using Bayesian inference over the set of possible images (Cohn-Gordon et al., 2018).",
        "reason": "Directly related prior that implements pragmatic captioning (including RSA variants) and provides baselines and methodological contrast for the proposed approach.",
        "category": "Medium"
      },
      {
        "rank": 9,
        "paperId": "88c86523d500d636f453647385ddaa04085b5f1b",
        "title": "Pragmatic Issue-Sensitive Image Captioning",
        "contexts": "…are chosen that have high probability for the target image and low probability for the distractor (Vedantam et al., 2017; Cohn-Gordon et al., 2018; Nie et al., 2020) or (2) using a separate discriminative model but selecting a discriminative caption from among a set of entire captions generated… | Our approach builds on a family of inferencetime pragmatic-based contrastive captioning methods which have taken one of two approaches: (1) incrementally generating captions but using only a captioning model (our speaker model), where tokens are chosen that have high probability for the target image and low probability for the distractor (Vedantam et al., 2017; Cohn-Gordon et al., 2018; Nie et al., 2020) or (2) using a separate discriminative model but selecting a discriminative caption from among a set of entire captions generated by the speaker model for the target image (Andreas and Klein, 2016; Luo and Shakhnarovich, 2017).",
        "reason": "Key prior pragmatic captioning approach representing the captioner-only family that PICL contrasts with.",
        "category": "Medium"
      },
      {
        "rank": 10,
        "paperId": "e782437503f2a24fd1a836a434da395bf15c88c2",
        "title": "Context-Aware Captions from Context-Agnostic Supervision",
        "contexts": "Past work on discriminative captioning has successfully applied techniques from computational pragmatics to trade off between the two criteria above (Andreas and Klein, 2016; Vedantam et al., 2017; Cohn-Gordon et al., 2018). | Our approach builds on a family of inferencetime pragmatic-based contrastive captioning methods which have taken one of two approaches: (1) incrementally generating captions but using only a captioning model (our speaker model), where tokens are chosen that have high probability for the target image and low probability for the distractor (Vedantam et al., 2017; Cohn-Gordon et al., 2018; Nie et al., 2020) or (2) using a separate discriminative model but selecting a discriminative caption from among a set of entire captions generated by the speaker model for the target image (Andreas and Klein, 2016; Luo and Shakhnarovich, 2017). | …model (our speaker model), where tokens are chosen that have high probability for the target image and low probability for the distractor (Vedantam et al., 2017; Cohn-Gordon et al., 2018; Nie et al., 2020) or (2) using a separate discriminative model but selecting a discriminative… | Prior work on contrastive/discriminative captioning investigates the tradeoff of model performance between discriminativeness and accuracy/fluency (Wang et al., 2021; Liu et al., 2019; Honda et al., 2022; Cho et al., 2022; Vedantam et al., 2017; Andreas and Klein, 2016).",
        "reason": "Represents prior discriminative/pragmatic captioning methodology that the paper situates itself against.",
        "category": "Medium"
      },
      {
        "rank": 11,
        "paperId": "b81fcb25cc5c4b0a850411fc6181eb96dd78b2b9",
        "title": "Colors in Context: A Pragmatic Neural Model for Grounded Language Understanding",
        "contexts": "Prior work has found that pragmatic generation can improve performance on a variety of NLP tasks, including reference games (Monroe et al., 2017), instruction generation (Fried et al. | Prior work has found that pragmatic generation can improve performance on a variety of NLP tasks, including reference games (Monroe et al., 2017), instruction generation (Fried et al., 2018), summarization (Shen et al., 2019), machine translation (Cohn-Gordon and Goodman, 2019), and dialogue (Kim…",
        "reason": "Canonical pragmatic modeling work for reference games that grounds the paper’s general approach.",
        "category": "Medium"
      },
      {
        "rank": 12,
        "paperId": "2a26153354119ba1e21c3c42050cb1546f886410",
        "title": "Predicting Pragmatic Reasoning in Language Games",
        "contexts": "Pragmatics Our approach to contrastive generation follows a long line of work on computational pragmatics, particularly in the Rational Speech Acts framework (Frank and Goodman, 2012; Goodman and Frank, 2016) which models language generation as an interaction between speakers and listeners.",
        "reason": "RSA-related work informing the paper’s speaker-listener reasoning formulation; helpful but not unique.",
        "category": "Medium"
      },
      {
        "rank": 13,
        "paperId": "38b0567e83386ddc294d6c81b541deacbd8e3c2a",
        "title": "CLIPScore: A Reference-free Evaluation Metric for Image Captioning",
        "contexts": "As shown in previous work, the rich vision-language representation learned in CLIP (1) provides robust assessments of modelgenerated captions that highly correlate with human judgments (Hessel et al., 2021), and (2) effectively quantifies the degree of discriminativeness/informativeness of visual referring expressions (Takmaz et al. | …representation learned in CLIP (1) provides robust assessments of model-generated captions that highly correlate with human judgments (Hessel et al., 2021), and (2) effectively quantifies the degree of discriminative-ness/informativeness of visual referring expressions (Takmaz et…",
        "reason": "Supports the choice of CLIP-based listener by evidencing CLIP’s strong alignment with human judgments; not strictly required for the method.",
        "category": "Medium"
      },
      {
        "rank": 14,
        "paperId": "448ef5b388464de0b75ba4c4ed03c845758d3c1f",
        "title": "Less Descriptive yet Discriminative: Quantifying the Properties of Multimodal Referring Utterances via CLIP",
        "contexts": ", 2021), and (2) effectively quantifies the degree of discriminativeness/informativeness of visual referring expressions (Takmaz et al., 2022). | …representation learned in CLIP (1) provides robust assessments of model-generated captions that highly correlate with human judgments (Hessel et al., 2021), and (2) effectively quantifies the degree of discriminative-ness/informativeness of visual referring expressions (Takmaz et al., 2022).",
        "reason": "Further evidence that CLIP can quantify discriminativeness, motivating the CLIP-listener design choice.",
        "category": "Medium"
      },
      {
        "rank": 15,
        "paperId": "c279ae3ae94803f1cdbaa68bbede7c4c709f48d0",
        "title": "Communication breakdown: On the low mutual intelligibility between human and neural captioning",
        "contexts": "Recent analysis on ImageCode (Dessì et al., 2022) and in other reference game settings (Lazaridou et al. | disfluent captions — identifying captions that are interpretable under our evaluating listener model, ALBEF, but potentially confusing to a human, consistent with past work identifying language drift in reference game setups (Lazaridou et al., 2020; Dessì et al., 2022). | …a “rationality” parameter. disfluent captions — identifying captions that are interpretable under our evaluating listener model, ALBEF, but potentially confusing to a human, consistent with past work identifying language drift in reference game setups (Lazaridou et al., 2020; Dessì et al., 2022). | Recent analysis on ImageCode (Dessì et al., 2022) and in other reference game settings (Lazaridou et al., 2020) reveals that utterances generated by neural models can be discriminative enough for other neural models to retrieve the target image while being misleading to humans. | It is challenging to automatically choose a value for this hyperparameter, as captions that appear to be discriminative under a captioning model are frequently uninformative for people (Dessì et al., 2022).",
        "reason": "Motivates human-model mismatch and robustness concerns central to the paper’s argument about choosing the rationality hyperparameter and evaluating informativeness.",
        "category": "Medium"
      },
      {
        "rank": 16,
        "paperId": "5931c8ac145baf17cec9effc25c051049b7dfd4c",
        "title": "Reference-Centric Models for Grounded Collaborative Dialogue",
        "contexts": "…in evaluation, past work has made the evaluative listener dissimilar by training it on separate data (Cohn-Gordon et al., 2018; Kim et al., 2019; Fried et al., 2021); we additionally use a separate architecture for the evaluative listener, dissimilar from our CLIP listener: the ALBEF… | , 2019), machine translation (Cohn-Gordon and Goodman, 2019), and dialogue (Kim et al., 2020; Fried et al., 2021). | …generation can improve performance on a variety of NLP tasks, including reference games (Monroe et al., 2017), instruction generation (Fried et al., 2018), summarization (Shen et al., 2019), machine translation (Cohn-Gordon and Goodman, 2019), and dialogue (Kim et al., 2020; Fried et al., 2021).",
        "reason": "Supports the evaluation choice to use a dissimilar, separately trained listener; conceptually helpful but not essential.",
        "category": "Medium"
      },
      {
        "rank": 17,
        "paperId": "58641a3a4b4653b5d63e57dc6dfe3935b866d78f",
        "title": "CoDraw: Collaborative Drawing as a Testbed for Grounded Goal-driven Communication",
        "contexts": "However, an evaluative listener can only be an imperfect proxy for human listeners, and past work has found that utterances that are informative to an evaluative listener model can be uninterpretable to people, a phenomenon known as codebooking (Kim et al., 2019) or language drift (Lazaridou et al. | …an evaluative listener can only be an imperfect proxy for human listeners, and past work has found that utterances that are informative to an evaluative listener model can be uninterpretable to people, a phenomenon known as codebooking (Kim et al., 2019) or language drift (Lazaridou et al., 2020). | …codebooking issue in evaluation, past work has made the evaluative listener dissimilar by training it on separate data (Cohn-Gordon et al., 2018; Kim et al., 2019; Fried et al., 2021); we additionally use a separate architecture for the evaluative listener, dissimilar from our CLIP listener:…",
        "reason": "Cited for language drift/codebooking and evaluation concerns; supports motivation rather than method.",
        "category": "Medium"
      },
      {
        "rank": 18,
        "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
        "title": "Language Models are Unsupervised Multitask Learners",
        "contexts": "For fluency, we score the well-formedness of generated captions with the perplexity (PPL) under GPT-2 (Radford et al., 2019). | Therefore, we additionally perform automatic evaluations of the fluency of generated captions by computing their perplexity using a GPT-2 language model (Radford et al., 2019).",
        "reason": "Operationally used to measure fluency via GPT-2 perplexity; useful but easily replaceable by other LMs.",
        "category": "Medium"
      },
      {
        "rank": 19,
        "paperId": "b6473852e19ebb31161b2f62d53912b431231fa5",
        "title": "Compare and Reweight: Distinctive Image Captioning Using Similar Images Sets",
        "contexts": "…captions that optimize for discriminative objectives, e.g., minimizing the textual similarity between captions for the target and distractor images (Wang et al., 2020), using generated captions as input to image retrieval models (Luo et al., 2018; Liu et al., 2018), and computing CLIP similarity… | , minimizing the textual similarity between captions for the target and distractor images (Wang et al., 2020), using generated captions as input to image retrieval models (Luo et al.",
        "reason": "Background on alternative discriminative captioning strategies; not central to the proposed pragmatic CLIP-listener approach.",
        "category": "Low"
      },
      {
        "rank": 20,
        "paperId": "5c43607c7f10284003e0072b8632ef7427d3df06",
        "title": "Switching to Discriminative Image Captioning by Relieving a Bottleneck of Reinforcement Learning",
        "contexts": " , 2019), and finetuning RL-optimized caption models to encourage low-frequency words (Honda et al., 2022). | …on similar and/or unique objects among target and distractors (Wang et al., 2021; Mao et al., 2022), para-phrasing generic captions to enhance both diversity and informativeness (Liu et al., 2019), and fine-tuning RL-optimized caption models to encourage low-frequency words (Honda et al., 2022). | Prior work on contrastive/discriminative captioning investigates the tradeoff of model performance between discriminativeness and accuracy/fluency (Wang et al., 2021; Liu et al., 2019; Honda et al., 2022; Cho et al., 2022; Vedantam et al., 2017; Andreas and Klein, 2016).",
        "reason": "Acknowledges alternative training-time discriminative methods; peripheral to the main inference-time pragmatic contribution.",
        "category": "Low"
      },
      {
        "rank": 21,
        "paperId": "5204247ea32e1901a01c6ac16aa92cd2659ea9b0",
        "title": "Rethinking the Reference-based Distinctive Image Captioning",
        "contexts": "…image regional features to generate distinctive captions based on similar and/or unique objects among target and distractors (Wang et al., 2021; Mao et al., 2022), para-phrasing generic captions to enhance both diversity and informativeness (Liu et al., 2019), and fine-tuning RL-optimized… | tractors (Wang et al., 2021; Mao et al., 2022), paraphrasing generic captions to enhance both diversity and informativeness (Liu et al.",
        "reason": "Background on distinctive captioning techniques; not directly used by the proposed method.",
        "category": "Low"
      },
      {
        "rank": 22,
        "paperId": "a63699905b26fe35f4f5b9b5e2872450a6187f98",
        "title": "Group-based Distinctive Image Captioning with Memory Attention",
        "contexts": "tractors (Wang et al., 2021; Mao et al., 2022), paraphrasing generic captions to enhance both diversity and informativeness (Liu et al. | …fine-grained image regional features to generate distinctive captions based on similar and/or unique objects among target and distractors (Wang et al., 2021; Mao et al., 2022), para-phrasing generic captions to enhance both diversity and informativeness (Liu et al., 2019), and… | Prior work on contrastive/discriminative captioning investigates the tradeoff of model performance between discriminativeness and accuracy/fluency (Wang et al., 2021; Liu et al., 2019; Honda et al., 2022; Cho et al., 2022; Vedantam et al., 2017; Andreas and Klein, 2016).",
        "reason": "Another line of distinctive captioning; cited for breadth and context rather than methodological dependence.",
        "category": "Low"
      },
      {
        "rank": 23,
        "paperId": "caab1c1d53718315f54bc4df42eb9a727fa18483",
        "title": "Show, Tell and Discriminate: Image Captioning by Self-retrieval with Partially Labeled Data",
        "contexts": ", 2020), using generated captions as input to image retrieval models (Luo et al., 2018; Liu et al., 2018), and computing CLIP similarity scores between captions and target images (Cho et al. | …minimizing the textual similarity between captions for the target and distractor images (Wang et al., 2020), using generated captions as input to image retrieval models (Luo et al., 2018; Liu et al., 2018), and computing CLIP similarity scores between captions and target images (Cho et al., 2022).",
        "reason": "Alternative discriminative training paradigm (self-retrieval); tangential to the paper’s inference-time pragmatic approach.",
        "category": "Low"
      },
      {
        "rank": 24,
        "paperId": "7c1802d8d43dfe783650a03f03d41609fa5ae91e",
        "title": "Discriminability Objective for Training Descriptive Captions",
        "contexts": ", 2020), using generated captions as input to image retrieval models (Luo et al., 2018; Liu et al., 2018), and computing CLIP similarity scores between captions and target images (Cho et al. | …minimizing the textual similarity between captions for the target and distractor images (Wang et al., 2020), using generated captions as input to image retrieval models (Luo et al., 2018; Liu et al., 2018), and computing CLIP similarity scores between captions and target images (Cho et al., 2022).",
        "reason": "Background on discriminability objectives; not directly necessary for the proposed inference method.",
        "category": "Low"
      },
      {
        "rank": 25,
        "paperId": "9b45e9a40313096abf530df3b98a1dfa1553f17b",
        "title": "Comprehension-Guided Referring Expressions",
        "contexts": ", 2020) or (2) using a separate discriminative model but selecting a discriminative caption from among a set of entire captions generated by the speaker model for the target image (Andreas and Klein, 2016; Luo and Shakhnarovich, 2017).",
        "reason": "Cited as an example of using a separate discriminative model; peripheral to the main contribution.",
        "category": "Low"
      },
      {
        "rank": 26,
        "paperId": "baf47cd0b471a9bb7b2230fec0b680fc9b3c4783",
        "title": "Unified Pragmatic Models for Generating and Following Instructions",
        "contexts": ", 2017), instruction generation (Fried et al., 2018), summarization (Shen et al. | …pragmatic generation can improve performance on a variety of NLP tasks, including reference games (Monroe et al., 2017), instruction generation (Fried et al., 2018), summarization (Shen et al., 2019), machine translation (Cohn-Gordon and Goodman, 2019), and dialogue (Kim et al., 2020; Fried et… | Thus, we adopt a sub-sampling approach similar to Andreas and Klein (2016); Fried et al. (2018).",
        "reason": "General pragmatic generation background and sampling strategy reference; not specific to this paper’s core method.",
        "category": "Low"
      },
      {
        "rank": 27,
        "paperId": "743d1aae44a12fb37b743ec947fad41cba9831b8",
        "title": "Pragmatically Informative Text Generation",
        "contexts": ", 2018), summarization (Shen et al., 2019), machine translation (Cohn-Gordon and Goodman, 2019), and dialogue (Kim et al. | …generation can improve performance on a variety of NLP tasks, including reference games (Monroe et al., 2017), instruction generation (Fried et al., 2018), summarization (Shen et al., 2019), machine translation (Cohn-Gordon and Goodman, 2019), and dialogue (Kim et al., 2020; Fried et al., 2021).",
        "reason": "Broad background on pragmatic generation across tasks; informative but not operationally required.",
        "category": "Low"
      },
      {
        "rank": 28,
        "paperId": "ed7c3eececad3915c865b7b11d88c338b0e0cbe1",
        "title": "Lost in Machine Translation: A Method to Reduce Meaning Loss",
        "contexts": ", 2019), machine translation (Cohn-Gordon and Goodman, 2019), and dialogue (Kim et al.",
        "reason": "Background on pragmatic methods in MT; peripheral to the paper’s focus on image captioning.",
        "category": "Low"
      },
      {
        "rank": 29,
        "paperId": "92e02bd58b99ac17b475081611f091f4b0776482",
        "title": "Video Storytelling: Textual Summaries for Events",
        "contexts": "…made the evaluative listener dissimilar by training it on separate data (Cohn-Gordon et al., 2018; Kim et al., 2019; Fried et al., 2021); we additionally use a separate architecture for the evaluative listener, dissimilar from our CLIP listener: the ALBEF vision-language model (Li et al., 2020).",
        "reason": "Cited in passing regarding evaluative listener setup; not directly relevant to the main method or experiments.",
        "category": "Low"
      }
    ]
  }
]