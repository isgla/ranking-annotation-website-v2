[
  {
    "paperId": "c6fa25f723a1a842b8452bc04d154e44186d05c1",
    "paperLink": "https://www.semanticscholar.org/paper/c6fa25f723a1a842b8452bc04d154e44186d05c1",
    "paperTitle": "Efficacy of Language Model Self-Play in Non-Zero-Sum Games",
    "candidates": [
      {
        "rank": 1,
        "paperId": "6a8dbea5e40831bd6e987c03b76487f45ac49599",
        "title": "Deal or No Deal? End-to-End Learning of Negotiation Dialogues",
        "contexts": "Game Setup Following Lewis et al. (2017), we present two players with a shared collection of books, hats, and balls (with 5-7 total objects). | A handful of grounded dialogue tasks are focused on bartering or negotiation, including Deal or No Deal (DoND; Lewis et al., 2017), CaSiNo (Chawla et al., 2021), and the fruit trading game from Gemp et al. (2024). | Self-Improving Language Models Lewis et al. (2017) trained GRU-based language models on the Deal or No Deal task using REINFORCE (Williams, 1992). | Although these results highlight potential room for improvement, we view them as a promising initial signal for self-play training of large language models and release Lewis et al. (2017). | This finding contradicts existing wisdom that self-play is ineffective in collaborative domains (Strouse et al., 2021), or that models need to be trained on task-specific human data to avoid divergence from human-interpretable language (Lewis et al., 2017; FAIR, 2022). | These scores are much lower than those obtained in prior work (Lewis et al., 2017; Gandhi et al., 2023) because we did not provide models with few-shot example dialogues or task-specific finetuning data. | …training agents to communicate via self-play has shown that they often invent unin-terpretable communication strategies (Kottur et al., 2017); even when initialized with natural language data, self-play can cause models to gradually di-verge from human-interpretable language (Lewis et al., 2017). | For initializing a new instance of the game, we sample from a list of 4,186 valid game contexts (shared item counts and private value functions for each player), provided by Lewis et al. (2017). | This modification allows us to convert Deal or No Deal into a cooperative or strictly competitive game. all code for our environments, models, and human data collection to support future research in this area: github.com/nickatomlin/lm-selfplay . | We run a series of experiments on a negotiation task known as Deal or No Deal (Lewis et al., 2017) and train language models for multiple rounds of self-play across three different objectives on this task, ranging from fully cooperative, to semi-competitive, to strictly competitive. | In contrast, Lewis et al. (2017) paid workers $0.10 per game and $0.05 in bonus pay only when workers achieved the maximum score of ten points . | To address this question, we conducted experiments on Deal or No Deal (DoND; Lewis et al., 2017), a two-player negotiation game in which players decide how to divide a shared pool of items through natural language dialogue. | When we applied LM self-play to Deal or No Deal under the strictly competitive objective, the model failed to improve its performance, instead learning strategies that adversely impacted its ability to perform outside of self-play. | In contrast to our work, Lewis et al. (2017) did not learn a model tabula rasa but instead interleaved reinforcement learning from self-play with supervised learning on task-specific data to avoid divergence from human-interpretable language. | For the semi-competitive objec-tive, we finetuned models on 300 nonzero scoring games from the original human-human dataset in Lewis et al. (2017). | Self-Play Human Eval Self-Play 10.8 10.5 5.6 3.6 6.1 5.5 Table 1: Mean scores of models initially finetuned on task-specific data, which was either generated using GPT-4 self-play or extracted from prior human experiments in Lewis et al. (2017).",
        "reason": "Core dataset, task, and game contexts; the experiments, evaluation, and even initialization directly depend on Lewis et al. (2017).",
        "category": "High"
      },
      {
        "rank": 2,
        "paperId": "2e6b6de08f459e2165b11ed8d2103916966b0fcf",
        "title": "Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback",
        "contexts": "Another closely related work is Fu et al. (2023), which uses self-play to refine language models for a bargaining task. | A wave of recent work has focused on meth-ods for autonomously improving large language models at training (Ouyang et al., 2022; Bai et al., 2022; Abdulhai et al., 2023) or inference (Shinn et al., 2023; Yao et al., 2023; Wu et al., 2024) time. | Further, while our work focuses on finetuning models, Fu et al. (2023) present models with in-context demonstrations of previous games and natural language feedback from a critic model (similar to Re-flexion (Shinn et al., 2023)), leading to less major performance improvements.",
        "reason": "Closest domain-specific prior on negotiation self-play; directly frames methodological contrast and baseline expectations for improvements.",
        "category": "Medium"
      },
      {
        "rank": 3,
        "paperId": "aed8f01122d1a89c43900e995c80bfda7936568e",
        "title": "Autonomous Evaluation and Refinement of Digital Agents",
        "contexts": "More closely related to our work is Pan et al. (2024), which iteratively trains models for device-control tasks using filtered behavior cloning; however, in contrast to our work, Pan et al. (2024) studies a single agent interacting with an environment, rather than multiple agents interacting with… | …related to our work is Pan et al. (2024), which iteratively trains models for device-control tasks using filtered behavior cloning; however, in contrast to our work, Pan et al. (2024) studies a single agent interacting with an environment, rather than multiple agents interacting with one another. | To apply similar methods not just in game-playing domains but in real-world scenarios, we anticipate that models will need to rely on feedback from general-purpose, learned reward models (e.g., as in Du et al., 2023; Pan et al., 2024).",
        "reason": "Methodologically central as prior on iterative filtered behavior cloning, highlighting similarities and differences (single-agent vs multi-agent).",
        "category": "Medium"
      },
      {
        "rank": 4,
        "paperId": "23dd78e424d32f6a48660dcd67ce994b8a7db8be",
        "title": "STaR: Bootstrapping Reasoning With Reasoning",
        "contexts": "We implement a straightforward algorithm for language model self-play based on filtered behavior cloning (filtered BC; Chen et al., 2020, 2021; Zelik-man et al., 2022).",
        "reason": "Canonical reference for filtered behavior cloning/bootstrapped filtering that underpins the training algorithm.",
        "category": "Medium"
      },
      {
        "rank": 5,
        "paperId": "b054fc685c3fa56459d5e49e4b42547164f8e024",
        "title": "BAIL: Best-Action Imitation Learning for Batch Deep Reinforcement Learning",
        "contexts": "We implement a straightforward algorithm for language model self-play based on filtered behavior cloning (filtered BC; Chen et al., 2020, 2021; Zelik-man et al., 2022).",
        "reason": "Provides the best-action/filtered imitation idea that motivates selecting high-quality trajectories for behavior cloning.",
        "category": "Medium"
      },
      {
        "rank": 6,
        "paperId": "9a8f8ad72678fc61b8fcb416103df0a344b24bc9",
        "title": "Collaborating with Humans without Human Data",
        "contexts": "Methods such as fic-titious self-play and population play have been proposed to address these issues (Heinrich et al., 2015; Strouse et al., 2021), but have primarily been applied in games without language components. | This finding contradicts existing wisdom that self-play is ineffective in collaborative domains (Strouse et al., 2021), or that models need to be trained on task-specific human data to avoid divergence from human-interpretable language (Lewis et al., 2017; FAIR, 2022). | However, in settings that involve collabora-* Equal contribution. tion with humans, self-play is no longer guaranteed to yield optimal policies (Strouse et al., 2021).",
        "reason": "Conceptually central: articulates the limitations of self-play in collaborative settings that this paper directly investigates and challenges.",
        "category": "Medium"
      },
      {
        "rank": 7,
        "paperId": "2e01fdebbc780d3667ec3bf87a44927f0d9c188a",
        "title": "Decision-Oriented Dialogue for Human-AI Collaboration",
        "contexts": "In many of these tasks, models are automatically evaluated via self-play, which serves as a proxy for human evaluation (Fried et al., 2021; Lin et al., 2024). | 1 Game Environment Akin to recent work on language agents (Abdulhai et al., 2023; Lin et al., 2024), we implement an OpenAI Gym-like (Brock-man et al., 2016) environment for evaluating language models on DoND. | In tasks such as Cards (Djalali et al., 2011; Potts, 2012), CerealBar (Suhr et al., 2019), OneCommon (Udagawa and Aizawa, 2019), and DialOp (Lin et al., 2024), two agents must collaborate via natural language dialogue to achieve a shared goal within an environment.",
        "reason": "Influences evaluation framing (self-play as proxy) and environment design choices for dialogue-based collaboration.",
        "category": "Medium"
      },
      {
        "rank": 8,
        "paperId": "1e672bf4d38a93c4c140ee208216425444368fa6",
        "title": "LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models",
        "contexts": "1 Game Environment Akin to recent work on language agents (Abdulhai et al., 2023; Lin et al., 2024), we implement an OpenAI Gym-like (Brock-man et al., 2016) environment for evaluating language models on DoND. | A wave of recent work has focused on meth-ods for autonomously improving large language models at training (Ouyang et al., 2022; Bai et al., 2022; Abdulhai et al., 2023) or inference (Shinn et al., 2023; Yao et al., 2023; Wu et al., 2024) time.",
        "reason": "Guides the OpenAI Gym-like environment implementation and situates the study within multi-turn agent evaluation trends.",
        "category": "Medium"
      },
      {
        "rank": 9,
        "paperId": "0bd07bcdede3fecef852556168471b0098223e9f",
        "title": "Provable Self-Play Algorithms for Competitive Reinforcement Learning",
        "contexts": "In certain types of 2p0s games, self-play is theoretically guaranteed to produce optimal policies, given sufficient model capacity and compute (Bai and Jin, 2020; Bai et al., 2020).",
        "reason": "Provides theoretical grounding for self-play guarantees in 2-player zero-sum games, framing the paper’s extension to non-zero-sum settings.",
        "category": "Medium"
      },
      {
        "rank": 10,
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search",
        "contexts": "It is an open question whether the same principles that led to the success of models like AlphaGo can be applied to language models. | By iteratively training on their own data from games of self-play, models like AlphaGo and AlphaZero were able to continue improving long past the threshold of human performance. | Many of the greatest achievements in artificial intelligence have occurred in two-player zero-sum (2p0s) games such as Go (Silver et al., 2016), chess (Silver et al., 2018), and heads-up poker (Brown and Sandholm, 2018). | Training agents against copies of themselves is a longstanding technique in reinforcement learning (Littman, 1994), popularized in the past decade by models like AlphaGo (Silver et al., 2016) and AlphaZero (Silver et al., 2017).",
        "reason": "Canonical exemplar of self-play’s success that motivates applying self-play to language tasks.",
        "category": "Medium"
      },
      {
        "rank": 11,
        "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
        "title": "Mastering the game of Go without human knowledge",
        "contexts": "By iteratively training on their own data from games of self-play, models like AlphaGo and AlphaZero were able to continue improving long past the threshold of human performance. | Training agents against copies of themselves is a longstanding technique in reinforcement learning (Littman, 1994), popularized in the past decade by models like AlphaGo (Silver et al., 2016) and AlphaZero (Silver et al., 2017).",
        "reason": "Reinforces the self-play paradigm that inspires the paper’s methodology.",
        "category": "Medium"
      },
      {
        "rank": 12,
        "paperId": "f9717d29840f4d8f1cc19d1b1e80c5d12ec40608",
        "title": "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play",
        "contexts": "Many of the greatest achievements in artificial intelligence have occurred in two-player zero-sum (2p0s) games such as Go (Silver et al., 2016), chess (Silver et al., 2018), and heads-up poker (Brown and Sandholm, 2018).",
        "reason": "Further evidence of self-play’s effectiveness in competitive games, providing conceptual backdrop.",
        "category": "Medium"
      },
      {
        "rank": 13,
        "paperId": "eaabb78d0bc44ed132e4d077e9486c86a9e4cda9",
        "title": "Superhuman AI for heads-up no-limit poker: Libratus beats top professionals",
        "contexts": "Many of the greatest achievements in artificial intelligence have occurred in two-player zero-sum (2p0s) games such as Go (Silver et al., 2016), chess (Silver et al., 2018), and heads-up poker (Brown and Sandholm, 2018).",
        "reason": "Part of the canonical set demonstrating self-play in 2p0s domains, grounding the paper’s motivation.",
        "category": "Medium"
      },
      {
        "rank": 14,
        "paperId": "eb41a0cffa84d3d6035e6f5f420806ddc962b1e6",
        "title": "On the Utility of Learning about Humans for Human-AI Coordination",
        "contexts": "Experiments on games such as Overcooked (Carroll et al., 2019) and Hanabi (Bard et al., 2020) have shown that policies learned via self-play often fail to generalize to collaborative or imperfect information games.",
        "reason": "Motivates the need to evaluate beyond pure self-play and to test human collaboration generalization.",
        "category": "Medium"
      },
      {
        "rank": 15,
        "paperId": "6a505dbfb89cf05344457bf85b2e8307af5c4ad0",
        "title": "The Hanabi Challenge: A New Frontier for AI Research",
        "contexts": "Experiments on games such as Overcooked (Carroll et al., 2019) and Hanabi (Bard et al., 2020) have shown that policies learned via self-play often fail to generalize to collaborative or imperfect information games.",
        "reason": "Provides concrete evidence of self-play’s limits in cooperative/imperfect-information games, directly relevant to the paper’s setting.",
        "category": "Medium"
      },
      {
        "rank": 16,
        "paperId": "e89ed6bb1864558e3889f5f2fb8931643c633479",
        "title": "Human-level play in the game of Diplomacy by combining language models with strategic reasoning",
        "contexts": "This finding contradicts existing wisdom that self-play is ineffective in collaborative domains (Strouse et al., 2021), or that models need to be trained on task-specific human data to avoid divergence from human-interpretable language (Lewis et al., 2017; FAIR, 2022).",
        "reason": "Related high-profile result on language-model agents in multi-agent strategic settings; supports broader feasibility claims.",
        "category": "Medium"
      },
      {
        "rank": 17,
        "paperId": "5931c8ac145baf17cec9effc25c051049b7dfd4c",
        "title": "Reference-Centric Models for Grounded Collaborative Dialogue",
        "contexts": "In many of these tasks, models are automatically evaluated via self-play, which serves as a proxy for human evaluation (Fried et al., 2021; Lin et al., 2024).",
        "reason": "Helps situate use of self-play as an evaluation proxy, but not essential to the method or experiments.",
        "category": "Low"
      },
      {
        "rank": 18,
        "paperId": "2caa021d85d4878d3369000e0068f617576d6cca",
        "title": "Natural Language Does Not Emerge ‘Naturally’ in Multi-Agent Dialog",
        "contexts": "Divergence issues abound in other settings where models are trained via self-play, including in emergent communication (Kottur et al., 2017; Lowe et al., 2019; Tomlin and Pavlick, 2019). | …previous work on training agents to communicate via self-play has shown that they often invent unin-terpretable communication strategies (Kottur et al., 2017); even when initialized with natural language data, self-play can cause models to gradually di-verge from human-interpretable…",
        "reason": "Background on language divergence under self-play; motivates caution but not operationally used.",
        "category": "Low"
      },
      {
        "rank": 19,
        "paperId": "429550495358547ee633a1ecdad1c200c1adb17b",
        "title": "Emergent Compositionality in Signaling Games",
        "contexts": "Divergence issues abound in other settings where models are trained via self-play, including in emergent communication (Kottur et al., 2017; Lowe et al., 2019; Tomlin and Pavlick, 2019).",
        "reason": "Background on emergent communication/divergence; not directly used in methodology.",
        "category": "Low"
      },
      {
        "rank": 20,
        "paperId": "32a5dcd8ef1b1c41bbb494a953dc4bb63e82b40d",
        "title": "Steering Language Models with Game-Theoretic Solvers",
        "contexts": "A handful of grounded dialogue tasks are focused on bartering or negotiation, including Deal or No Deal (DoND; Lewis et al., 2017), CaSiNo (Chawla et al., 2021), and the fruit trading game from Gemp et al. (2024).",
        "reason": "Lists related negotiation/grounded-dialogue tasks; serves as contextual background.",
        "category": "Low"
      },
      {
        "rank": 21,
        "paperId": "37607828cb9b8ae5011bbcc8ecc2e159f719347c",
        "title": "CaSiNo: A Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems",
        "contexts": "A handful of grounded dialogue tasks are focused on bartering or negotiation, including Deal or No Deal (DoND; Lewis et al., 2017), CaSiNo (Chawla et al., 2021), and the fruit trading game from Gemp et al. (2024).",
        "reason": "Alternative negotiation dataset cited for context; not used in experiments.",
        "category": "Low"
      },
      {
        "rank": 22,
        "paperId": "be2ce82730600d9b2eb2df9f2762f9d4beb6222d",
        "title": "Executing Instructions in Situated Collaborative Interactions",
        "contexts": "In tasks such as Cards (Djalali et al., 2011; Potts, 2012), CerealBar (Suhr et al., 2019), OneCommon (Udagawa and Aizawa, 2019), and DialOp (Lin et al., 2024), two agents must collaborate via natural language dialogue to achieve a shared goal within an environment.",
        "reason": "Background on collaborative grounded-dialogue tasks; not central to the method.",
        "category": "Low"
      },
      {
        "rank": 23,
        "paperId": "ad7d5e5cea44c60605f742509eebe8a22502ffa9",
        "title": "A Natural Language Corpus of Common Grounding under Continuous and Partially-Observable Context",
        "contexts": "In tasks such as Cards (Djalali et al., 2011; Potts, 2012), CerealBar (Suhr et al., 2019), OneCommon (Udagawa and Aizawa, 2019), and DialOp (Lin et al., 2024), two agents must collaborate via natural language dialogue to achieve a shared goal within an environment.",
        "reason": "Contextual reference to collaborative dialogue benchmarks; not used in this work.",
        "category": "Low"
      },
      {
        "rank": 24,
        "paperId": "40e7a2cf08b04d1714b72ff3321160fdb46fa073",
        "title": "Modeling Expert Effects and Common Ground Using Questions Under Discussion",
        "contexts": "In tasks such as Cards (Djalali et al., 2011; Potts, 2012), CerealBar (Suhr et al., 2019), OneCommon (Udagawa and Aizawa, 2019), and DialOp (Lin et al., 2024), two agents must collaborate via natural language dialogue to achieve a shared goal within an environment.",
        "reason": "Background on collaborative language tasks; peripheral to the core contribution.",
        "category": "Low"
      },
      {
        "rank": 25,
        "paperId": "2352b3a6e4adc7b010ee5de424079480e3afbfbf",
        "title": "Goal-driven Answers in the Cards Dialogue Corpus",
        "contexts": "In tasks such as Cards (Djalali et al., 2011; Potts, 2012), CerealBar (Suhr et al., 2019), OneCommon (Udagawa and Aizawa, 2019), and DialOp (Lin et al., 2024), two agents must collaborate via natural language dialogue to achieve a shared goal within an environment.",
        "reason": "Another collaborative task reference for context only.",
        "category": "Low"
      },
      {
        "rank": 26,
        "paperId": "ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5",
        "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models",
        "contexts": "Chen et al. (2024) propose a method called self-play fine-tuning, but their usage of the term self-play differs from the traditional meaning, i.e., it does not involve agents interacting within an environment; instead, Chen et al. (2024) proposes a preference learning method akin to DPO (Rafailov… | …their usage of the term self-play differs from the traditional meaning, i.e., it does not involve agents interacting within an environment; instead, Chen et al. (2024) proposes a preference learning method akin to DPO (Rafailov et al., 2023) or PPO for reinforcement learning from human feedback…",
        "reason": "Cited to clarify terminology and distinguish from preference-learning ‘self-play’; not used methodologically.",
        "category": "Low"
      },
      {
        "rank": 27,
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
        "contexts": "Chen et al. (2024) propose a method called self-play fine-tuning, but their usage of the term self-play differs from the traditional meaning, i.e., it does not involve agents interacting within an environment; instead, Chen et al. (2024) proposes a preference learning method akin to DPO (Rafailov et al., 2023) or PPO for reinforcement learning from human feedback (Ouyang et al., 2022). | …term self-play differs from the traditional meaning, i.e., it does not involve agents interacting within an environment; instead, Chen et al. (2024) proposes a preference learning method akin to DPO (Rafailov et al., 2023) or PPO for reinforcement learning from human feedback (Ouyang et al., 2022).",
        "reason": "Background to contrast preference-learning approaches with environment-based self-play; not central to results.",
        "category": "Low"
      },
      {
        "rank": 28,
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback",
        "contexts": "5-turbo-0125 ; Chen et al., 2021; Ouyang et al., 2022) which strikes a balance between capability and accessibility. | …term self-play differs from the traditional meaning, i.e., it does not involve agents interacting within an environment; instead, Chen et al. (2024) proposes a preference learning method akin to DPO (Rafailov et al., 2023) or PPO for reinforcement learning from human feedback (Ouyang et al., 2022). | A wave of recent work has focused on meth-ods for autonomously improving large language models at training (Ouyang et al., 2022; Bai et al., 2022; Abdulhai et al., 2023) or inference (Shinn et al., 2023; Yao et al., 2023; Wu et al., 2024) time.",
        "reason": "Used to justify model choice and to contextualize RLHF vs self-play; helpful but replaceable.",
        "category": "Low"
      },
      {
        "rank": 29,
        "paperId": "1d2f541bc72544c1f38f39938550029a76cc6db6",
        "title": "Policy Learning with a Language Bottleneck",
        "contexts": "Another possible approach is described by Srivastava et al. (2024), in which a language model is used to describe distributional differences between good and bad trajectories.",
        "reason": "Referenced as an alternative approach; not used in this work.",
        "category": "Low"
      },
      {
        "rank": 30,
        "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
        "contexts": "A wave of recent work has focused on meth-ods for autonomously improving large language models at training (Ouyang et al., 2022; Bai et al., 2022; Abdulhai et al., 2023) or inference (Shinn et al., 2023; Yao et al., 2023; Wu et al., 2024) time.",
        "reason": "Part of broader landscape of autonomous improvement; included for context only.",
        "category": "Low"
      },
      {
        "rank": 31,
        "paperId": "0671fd553dd670a4e820553a974bc48040ba0819",
        "title": "Reflexion: language agents with verbal reinforcement learning",
        "contexts": "Future work may be able to obtain even larger improvements by combining self-play with approaches other than filtered BC, such as natural language reflections, e.g., akin to Shinn et al. (2023). | A wave of recent work has focused on meth-ods for autonomously improving large language models at training (Ouyang et al., 2022; Bai et al., 2022; Abdulhai et al., 2023) or inference (Shinn et al., 2023; Yao et al., 2023; Wu et al., 2024) time. | Further, while our work focuses on finetuning models, Fu et al. (2023) present models with in-context demonstrations of previous games and natural language feedback from a critic model (similar to Re-flexion (Shinn et al., 2023)), leading to less major performance improvements.",
        "reason": "Mentioned as a complementary direction and in related work; not essential to the study.",
        "category": "Low"
      },
      {
        "rank": 32,
        "paperId": "5278a8eb2ba2429d4029745caf4e661080073c81",
        "title": "Generative Agents: Interactive Simulacra of Human Behavior",
        "contexts": "Furthermore, self-play with pretrained language models might actually function more similarly to population play, since large language models are trained on text from a population of users and may simulate different personas in different contexts (Pataranutaporn et al., 2021; Park et al., 2023).",
        "reason": "Used to motivate population-play analogy; peripheral to methods and results.",
        "category": "Low"
      },
      {
        "rank": 33,
        "paperId": "8c7ed130efb8dd61213784ec9e88f7681944a40a",
        "title": "AI-generated characters for supporting personalized learning and well-being",
        "contexts": "Furthermore, self-play with pretrained language models might actually function more similarly to population play, since large language models are trained on text from a population of users and may simulate different personas in different contexts (Pataranutaporn et al., 2021; Park et al., 2023).",
        "reason": "Persona/population citation supporting a high-level analogy; not operationally used.",
        "category": "Low"
      },
      {
        "rank": 34,
        "paperId": "cf41ae462687f81ce95b27113c6a4f9c2751de42",
        "title": "Vision-Language Models as Success Detectors",
        "contexts": "To apply similar methods not just in game-playing domains but in real-world scenarios, we anticipate that models will need to rely on feedback from general-purpose, learned reward models (e.g., as in Du et al., 2023; Pan et al., 2024).",
        "reason": "Future direction for reward modeling; not used in current experiments.",
        "category": "Low"
      },
      {
        "rank": 35,
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback",
        "contexts": "A wave of recent work has focused on meth-ods for autonomously improving large language models at training (Ouyang et al., 2022; Bai et al., 2022; Abdulhai et al., 2023) or inference (Shinn et al., 2023; Yao et al., 2023; Wu et al., 2024) time.",
        "reason": "Background on autonomy/AI feedback; not specific to this paper’s method.",
        "category": "Low"
      },
      {
        "rank": 36,
        "paperId": "2264c5b9c8fe7c1baab1d7fe0988bce8acaf2fc5",
        "title": "Pragmatics in Language Grounding: Phenomena, Tasks, and Modeling Approaches",
        "contexts": "Fried et al. (2023) provides additional discussion of collaborative and competitive grounded dialogue tasks and modeling approaches.",
        "reason": "Survey-level context on grounded dialogue; not required for understanding the method.",
        "category": "Low"
      },
      {
        "rank": 37,
        "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
        "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling",
        "contexts": "We implement a straightforward algorithm for language model self-play based on filtered behavior cloning (filtered BC; Chen et al., 2020, 2021; Zelik-man et al., 2022). | 5-turbo-0125 ; Chen et al., 2021; Ouyang et al., 2022) which strikes a balance between capability and accessibility.",
        "reason": "Mentioned in passing in a citation bundle; not substantively used in the approach.",
        "category": "Low"
      },
      {
        "rank": 38,
        "paperId": "f1a3cd5cc340f47e3e966709f7dfddef23460aa2",
        "title": "Strategic Reasoning with Language Models",
        "contexts": "Because our goal is to analyze the effects of self-play on language model behavior, we do not prompt our models with few-shot example dialogues or finetune them on task-specific data; doing so helps us avoid biasing models toward specific patterns of behavior.",
        "reason": "Cited to justify avoiding few-shot/task-specific finetuning; minor influence.",
        "category": "Low"
      },
      {
        "rank": 39,
        "paperId": "8b20f103c1f20074fa35bd8fc41983964283acac",
        "title": "Fictitious Self-Play in Extensive-Form Games",
        "contexts": "Methods such as fic-titious self-play and population play have been proposed to address these issues (Heinrich et al., 2015; Strouse et al., 2021), but have primarily been applied in games without language components.",
        "reason": "Background on alternative self-play strategies; not applied here.",
        "category": "Low"
      },
      {
        "rank": 40,
        "paperId": "7fbf55baccbc5fdc7ded1ba18330605909aef5e5",
        "title": "Markov Games as a Framework for Multi-Agent Reinforcement Learning",
        "contexts": "Training agents against copies of themselves is a longstanding technique in reinforcement learning (Littman, 1994), popularized in the past decade by models like AlphaGo (Silver et al., 2016) and AlphaZero (Silver et al., 2017).",
        "reason": "Historical reference for self-play in RL; non-essential background.",
        "category": "Low"
      },
      {
        "rank": 41,
        "paperId": "6f9f9e9cb90e0139480fe983609f8e88556ee04f",
        "title": "Incorporating Worker Perspectives into MTurk Annotation Practices for NLP",
        "contexts": "We restricted our task to workers from the United States with a 98+% HIT approval rate and at least 500 completed HITs, based on recommendations in Huang et al. (2023).",
        "reason": "Guides crowdworker selection; peripheral to the technical contribution.",
        "category": "Low"
      },
      {
        "rank": 42,
        "paperId": "b626754a0fd7de12c87e88165b2484ac5d98212a",
        "title": "Decoupling Strategy and Generation in Negotiation Dialogues",
        "contexts": "In the Craigslist Bargaining task (He et al., 2018), agents negotiate on the price of a object for sale.",
        "reason": "Background on another negotiation setting; not used in this study.",
        "category": "Low"
      },
      {
        "rank": 43,
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning",
        "contexts": "Self-Improving Language Models Lewis et al. (2017) trained GRU-based language models on the Deal or No Deal task using REINFORCE (Williams, 1992).",
        "reason": "Cited indirectly via prior work on DoND; not influential for the present method.",
        "category": "Low"
      }
    ]
  }
]