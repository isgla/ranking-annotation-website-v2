[
  {
    "paperId": "c43cc65148bf7221ff035c75621b663371fe8250",
    "paperLink": "https://www.semanticscholar.org/paper/c43cc65148bf7221ff035c75621b663371fe8250",
    "paperTitle": "Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever",
    "chunk": "1",
    "candidates": [
      {
        "rank": "1",
        "paperId": "60b8ad6177230ad5402af409a6edb5af441baeb4",
        "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT",
        "reason": "Foundational late interaction architecture that Jina-ColBERT-v2 directly builds upon; establishes token-level multi-vector scoring central to this work."
      },
      {
        "rank": "2",
        "paperId": "590432f953b6ce1b4b36bf66a2ac65eeee567515",
        "title": "ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction",
        "reason": "Immediate predecessor providing efficiency, indexing, and search optimizations that Jina-ColBERT-v2 leverages and extends."
      },
      {
        "rank": "3",
        "paperId": "c74e7d696e0fc1a0cab14787a02808e301d0e0ba",
        "title": "ColBERT-XM: A Modular Multi-Vector Representation Model for Zero-Shot Multilingual Information Retrieval",
        "reason": "Key prior work on multilingual ColBERT-style retrievers informing Jina-ColBERT-v2’s multilingual design and comparisons."
      },
      {
        "rank": "4",
        "paperId": "e22ae34ea102a781d0494e115639e8d081bf6920",
        "title": "Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents",
        "reason": "Provides the multi-stage training recipe (large-scale pairwise contrastive pretraining + triplet fine-tuning) and long-context engineering lineage reused in this paper."
      },
      {
        "rank": "5",
        "paperId": "84109e1235b725f4bb44a54bab8b493bd723fdd3",
        "title": "Towards General Text Embeddings with Multi-stage Contrastive Learning",
        "reason": "Empirically motivates the two-stage contrastive training strategy the paper adopts to boost retrieval quality."
      },
      {
        "rank": "6",
        "paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0",
        "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
        "reason": "Enables efficient 8k-token context in the XLM-RoBERTa backbone, a core architectural change for long-context retrieval."
      },
      {
        "rank": "7",
        "paperId": "7a1e71cb1310c4a873e7a4e54d1a6dab0553adce",
        "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data",
        "reason": "and Web Data Only, Used to continue pretraining and warm up new rotary positional embeddings, critical for stable long-context support."
      },
      {
        "rank": "8",
        "paperId": "6f7efb07a1907195136039396277c8cb203b1f2e",
        "title": "Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings",
        "reason": "Prior bilingual/long-context embedding work from the same line that informs data curation (translations, synthetic sets) and training choices."
      }
    ]
  },
  {
    "paperId": "c43cc65148bf7221ff035c75621b663371fe8250",
    "paperLink": "https://www.semanticscholar.org/paper/c43cc65148bf7221ff035c75621b663371fe8250",
    "paperTitle": "Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever",
    "chunk": "2",
    "candidates": [
      {
        "rank": "9",
        "paperId": "dd95f96e3322dcaee9b1e3f7871ecc3ebcd51bfe",
        "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset",
        "reason": "Core supervised resource for triplet fine-tuning and benchmarking; also underpins mMARCO used for multilingual training/eval."
      },
      {
        "rank": "10",
        "paperId": "9e3facfdf48fc6fbdeab602647f360ceaf9c6313",
        "title": "MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages",
        "reason": "Central multilingual benchmark and part of the curated triplet data; key to demonstrating non-English retrieval performance."
      },
      {
        "rank": "11",
        "paperId": "972808b92159a782f5ca1c1ab3a8ed3867acfb2d",
        "title": "mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset",
        "reason": "Widely used machine-translated multilingual corpus the paper evaluates on and references for multilingual training."
      },
      {
        "rank": "12",
        "paperId": "1b09222cfe10f11c4cb0b18a9727d2baf6b991ac",
        "title": "Mr. TyDi: A Multi-lingual Benchmark for Dense Retrieval",
        "reason": "Influences multilingual evaluation and dataset framing; referenced in discussing human-authored multilingual benchmarks."
      },
      {
        "rank": "13",
        "paperId": "4d5735c186ddb2430ac9689ccf61fdcbbfc23abc",
        "title": "BGE M3-Embedding: Multi-Lingual",
        "reason": "Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation, A contemporary multilingual multi-vector baseline informing comparisons and design trade-offs (e.g., token representation size)."
      },
      {
        "rank": "14",
        "paperId": "85ceb93a797481b5203b36f4e77a0828107c42fd",
        "title": "Multilingual E5 Text Embeddings: A Technical Report",
        "reason": "Strong multilingual single-vector baseline shaping instruction usage and synthetic/translated data practices referenced by the paper."
      },
      {
        "rank": "15",
        "paperId": "943487997ecd26e871a2ab16160bd5640020369d",
        "title": "Toward Best Practices for Training Multilingual Dense Retrieval Models",
        "reason": "Guides training/evaluation choices for multilingual dense retrieval; used as a comparative yardstick."
      },
      {
        "rank": "16",
        "paperId": "d1ccffb8eb1b7a99cd586723074b82fa5399bdd2",
        "title": "Transfer Learning Approaches for Building Cross-Language Dense Retrieval Models",
        "reason": "Provides cross-lingual transfer strategies relevant to the paper’s multilingual training framework and comparisons."
      }
    ]
  },
  {
    "paperId": "c43cc65148bf7221ff035c75621b663371fe8250",
    "paperLink": "https://www.semanticscholar.org/paper/c43cc65148bf7221ff035c75621b663371fe8250",
    "paperTitle": "Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever",
    "chunk": "3",
    "candidates": [
      {
        "rank": "17",
        "paperId": "995b7affd684b910d5a1c520c3af00fd20cc39b0",
        "title": "DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications",
        "reason": "Included in the high-quality, human-annotated triplet training mixture, contributing to multilingual robustness."
      },
      {
        "rank": "18",
        "paperId": "63c0dbe2426f9c92f469151d1773e5265ae6580e",
        "title": "One Embedder",
        "reason": "Any Task: Instruction-Finetuned Text Embeddings, Inspires the paper’s experiments with task-specific natural language instructions for queries/documents."
      },
      {
        "rank": "19",
        "paperId": "b26f2037f769d5ffc5f7bdcec2de8da28ec14bee",
        "title": "Dense Passage Retrieval for Open-Domain Question Answering",
        "reason": "Canonical dense retrieval baseline providing context and contrast to late-interaction approaches."
      },
      {
        "rank": "20",
        "paperId": "c9b8593db099869fe7254aa1fa53f3c9073b0176",
        "title": "Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval",
        "reason": "Background on dense retriever training and negatives; contextualizes monolingual advances the paper moves beyond."
      },
      {
        "rank": "21",
        "paperId": "1e8a6de5561f557ff9abf43d538d8d5e9347efa0",
        "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking",
        "reason": "Representative sparse baseline used for conceptual contrast with multi-vector neural retrieval."
      },
      {
        "rank": "22",
        "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "reason": "Underpins masked language modeling and whole-word masking choices during continued pretraining."
      },
      {
        "rank": "23",
        "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
        "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "reason": "General PLM foundation relevant via the XLM-RoBERTa backbone used in this work."
      },
      {
        "rank": "24",
        "paperId": "6be8d23a906b62550d65586aa028240c99b60b8c",
        "title": "JaColBERTv2.5: Optimising Multi-Vector Retrievers to Create State-of-the-Art Japanese Retrievers with Constrained Resources",
        "reason": "Related ColBERT variant cited mainly for evaluation protocol (sampled-corpus comparisons); limited direct impact on architecture/training."
      }
    ]
  },
  {
    "paperId": "c43cc65148bf7221ff035c75621b663371fe8250",
    "paperLink": "https://www.semanticscholar.org/paper/c43cc65148bf7221ff035c75621b663371fe8250",
    "paperTitle": "Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever",
    "chunk": "4",
    "candidates": [
      {
        "rank": "25",
        "paperId": "0c5dbff43777b5f9eebc9ce3b04a4a23493e1f86",
        "title": "Neural Approaches to Multilingual Information Retrieval",
        "reason": "Broad survey providing background context on multilingual IR; minimal direct methodological influence."
      }
    ]
  }
]