[
  {
    "paperId": "c6fa25f723a1a842b8452bc04d154e44186d05c1",
    "paperLink": "https://www.semanticscholar.org/paper/c6fa25f723a1a842b8452bc04d154e44186d05c1",
    "paperTitle": "Efficacy of Language Model Self-Play in Non-Zero-Sum Games",
    "chunk": "1",
    "candidates": [
      {
        "rank": "1",
        "paperId": "6a8dbea5e40831bd6e987c03b76487f45ac49599",
        "title": "Deal or No Deal? End-to-End Learning of Negotiation Dialogues",
        "reason": "Core experimental domain: provides the game, dataset, contexts, initialization, and human data used in semi-competitive training; repeatedly cited for setup, divergence concerns, and baselines."
      },
      {
        "rank": "2",
        "paperId": "23dd78e424d32f6a48660dcd67ce994b8a7db8be",
        "title": "STaR: Bootstrapping Reasoning With Reasoning",
        "reason": "Direct methodological influence: foundation for the “filtered behavior cloning” training loop used in LM self-play."
      },
      {
        "rank": "3",
        "paperId": "b054fc685c3fa56459d5e49e4b42547164f8e024",
        "title": "BAIL: Best-Action Imitation Learning for Batch Deep Reinforcement Learning",
        "reason": "Key source for filtered BC idea of selecting high-reward trajectories to imitate."
      },
      {
        "rank": "4",
        "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
        "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling",
        "reason": "Part of the filtered BC lineage informing sequence-model imitation of top trajectories."
      },
      {
        "rank": "5",
        "paperId": "aed8f01122d1a89c43900e995c80bfda7936568e",
        "title": "Autonomous Evaluation and Refinement of Digital Agents",
        "reason": "Closely related iterative filtered-BC training; contrasted directly as single-agent vs multi-agent interaction."
      },
      {
        "rank": "6",
        "paperId": "2e6b6de08f459e2165b11ed8d2103916966b0fcf",
        "title": "Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback",
        "reason": "Domain-closest prior on bargaining with LM self-play; provides comparison point (ICL + critic vs finetuning)."
      },
      {
        "rank": "7",
        "paperId": "9a8f8ad72678fc61b8fcb416103df0a344b24bc9",
        "title": "Collaborating with Humans without Human Data",
        "reason": "Central theoretical framing: self-play not guaranteed optimal in human collaboration; motivates population/fictitious self-play and is explicitly “contradicted” by findings."
      },
      {
        "rank": "8",
        "paperId": "eb41a0cffa84d3d6035e6f5f420806ddc962b1e6",
        "title": "On the Utility of Learning about Humans for Human-AI Coordination",
        "reason": "Evidence that self-play policies often don’t generalize to humans (Overcooked/Hanabi); motivates evaluating with humans."
      }
    ]
  },
  {
    "paperId": "c6fa25f723a1a842b8452bc04d154e44186d05c1",
    "paperLink": "https://www.semanticscholar.org/paper/c6fa25f723a1a842b8452bc04d154e44186d05c1",
    "paperTitle": "Efficacy of Language Model Self-Play in Non-Zero-Sum Games",
    "chunk": "2",
    "candidates": [
      {
        "rank": "9",
        "paperId": "0bd07bcdede3fecef852556168471b0098223e9f",
        "title": "Provable Self-Play Algorithms for Competitive Reinforcement Learning",
        "reason": "Provides the theoretical guarantees for self-play in 2-player zero-sum settings that the paper extends beyond."
      },
      {
        "rank": "10",
        "paperId": "e89ed6bb1864558e3889f5f2fb8931643c633479",
        "title": "Human-level play in the game of Diplomacy by combining language models with strategic reasoning",
        "reason": "Context for LMs in complex multi-agent games and discussion that challenges “self-play ineffective in collaboration” wisdom."
      },
      {
        "rank": "11",
        "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
        "title": "Mastering the game of Go without human knowledge",
        "reason": "Canonical self-play success motivating the approach; repeatedly referenced as background."
      },
      {
        "rank": "12",
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search",
        "reason": "Foundational self-play result motivating investigation in language settings."
      },
      {
        "rank": "13",
        "paperId": "eaabb78d0bc44ed132e4d077e9486c86a9e4cda9",
        "title": "Superhuman AI for heads-up no-limit poker: Libratus beats top professionals",
        "reason": "Landmark 2p0s self-play success used as broader motivation."
      },
      {
        "rank": "14",
        "paperId": "f9717d29840f4d8f1cc19d1b1e80c5d12ec40608",
        "title": "A general reinforcement learning algorithm that masters chess",
        "reason": "shogi, and Go through self-play, Additional general self-play success motivating the study."
      },
      {
        "rank": "15",
        "paperId": "6a505dbfb89cf05344457bf85b2e8307af5c4ad0",
        "title": "The Hanabi Challenge: A New Frontier for AI Research",
        "reason": "Cited to highlight self-play-to-human generalization difficulties in cooperative/imperfect information games."
      },
      {
        "rank": "16",
        "paperId": "2e01fdebbc780d3667ec3bf87a44927f0d9c188a",
        "title": "Decision-Oriented Dialogue for Human-AI Collaboration",
        "reason": "Influenced environment framing and the use of self-play as an automatic evaluation proxy in collaborative tasks."
      }
    ]
  },
  {
    "paperId": "c6fa25f723a1a842b8452bc04d154e44186d05c1",
    "paperLink": "https://www.semanticscholar.org/paper/c6fa25f723a1a842b8452bc04d154e44186d05c1",
    "paperTitle": "Efficacy of Language Model Self-Play in Non-Zero-Sum Games",
    "chunk": "3",
    "candidates": [
      {
        "rank": "17",
        "paperId": "1e672bf4d38a93c4c140ee208216425444368fa6",
        "title": "LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models",
        "reason": "Inspiration for an OpenAI Gym-like setup for LM environments akin to recent language agent work."
      },
      {
        "rank": "18",
        "paperId": "f1a3cd5cc340f47e3e966709f7dfddef23460aa2",
        "title": "Strategic Reasoning with Language Models",
        "reason": "Informed the experimental design choice to avoid few-shot/task-specific finetuning to isolate self-play effects."
      },
      {
        "rank": "19",
        "paperId": "2caa021d85d4878d3369000e0068f617576d6cca",
        "title": "Natural Language Does Not Emerge ‘Naturally’ in Multi-Agent Dialog",
        "reason": "Supports concerns about language drift/divergence under self-play; motivates careful training/evaluation."
      },
      {
        "rank": "20",
        "paperId": "429550495358547ee633a1ecdad1c200c1adb17b",
        "title": "Emergent Compositionality in Signaling Games",
        "reason": "Background on emergent communication divergence that frames risks of self-play without constraints."
      },
      {
        "rank": "21",
        "paperId": "32a5dcd8ef1b1c41bbb494a953dc4bb63e82b40d",
        "title": "Steering Language Models with Game-Theoretic Solvers",
        "reason": "Situates DoND among negotiation/barter tasks; links LMs and game-theoretic perspectives."
      },
      {
        "rank": "22",
        "paperId": "37607828cb9b8ae5011bbcc8ecc2e159f719347c",
        "title": "CaSiNo: A Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems",
        "reason": "Alternative negotiation benchmark used to contextualize task choice."
      },
      {
        "rank": "23",
        "paperId": "b626754a0fd7de12c87e88165b2484ac5d98212a",
        "title": "Decoupling Strategy and Generation in Negotiation Dialogues",
        "reason": "Related negotiation literature (e.g., Craigslist Bargaining) situating the work."
      },
      {
        "rank": "24",
        "paperId": "2264c5b9c8fe7c1baab1d7fe0988bce8acaf2fc5",
        "title": "Pragmatics in Language Grounding: Phenomena",
        "reason": "Tasks, and Modeling Approaches, Survey grounding the broader landscape of collaborative/competitive grounded dialogue."
      }
    ]
  },
  {
    "paperId": "c6fa25f723a1a842b8452bc04d154e44186d05c1",
    "paperLink": "https://www.semanticscholar.org/paper/c6fa25f723a1a842b8452bc04d154e44186d05c1",
    "paperTitle": "Efficacy of Language Model Self-Play in Non-Zero-Sum Games",
    "chunk": "4",
    "candidates": [
      {
        "rank": "25",
        "paperId": "5931c8ac145baf17cec9effc25c051049b7dfd4c",
        "title": "Reference-Centric Models for Grounded Collaborative Dialogue",
        "reason": "Supports practice of self-play evaluation as a proxy for human studies in grounded dialogue."
      },
      {
        "rank": "26",
        "paperId": "be2ce82730600d9b2eb2df9f2762f9d4beb6222d",
        "title": "Executing Instructions in Situated Collaborative Interactions",
        "reason": "Background on collaborative language tasks within environments."
      },
      {
        "rank": "27",
        "paperId": "ad7d5e5cea44c60605f742509eebe8a22502ffa9",
        "title": "A Natural Language Corpus of Common Grounding under Continuous and Partially-Observable Context",
        "reason": "Additional related collaborative dialogue benchmark context."
      },
      {
        "rank": "28",
        "paperId": "40e7a2cf08b04d1714b72ff3321160fdb46fa073",
        "title": "Modeling Expert Effects and Common Ground Using Questions Under Discussion",
        "reason": "Related task demonstrating collaborative dialogue requirements."
      },
      {
        "rank": "29",
        "paperId": "2352b3a6e4adc7b010ee5de424079480e3afbfbf",
        "title": "Goal-driven Answers in the Cards Dialogue Corpus",
        "reason": "Another collaborative dialogue benchmark grounding the task landscape."
      },
      {
        "rank": "30",
        "paperId": "6f9f9e9cb90e0139480fe983609f8e88556ee04f",
        "title": "Incorporating Worker Perspectives into MTurk Annotation Practices for NLP",
        "reason": "Informed the MTurk worker qualification criteria used in human evaluations."
      },
      {
        "rank": "31",
        "paperId": "7fbf55baccbc5fdc7ded1ba18330605909aef5e5",
        "title": "Markov Games as a Framework for Multi-Agent Reinforcement Learning",
        "reason": "Foundational background for self-play in multi-agent RL."
      },
      {
        "rank": "32",
        "paperId": "8b20f103c1f20074fa35bd8fc41983964283acac",
        "title": "Fictitious Self-Play in Extensive-Form Games",
        "reason": "Alternative self-play/population-play approach cited when discussing mitigation strategies."
      }
    ]
  },
  {
    "paperId": "c6fa25f723a1a842b8452bc04d154e44186d05c1",
    "paperLink": "https://www.semanticscholar.org/paper/c6fa25f723a1a842b8452bc04d154e44186d05c1",
    "paperTitle": "Efficacy of Language Model Self-Play in Non-Zero-Sum Games",
    "chunk": "5",
    "candidates": [
      {
        "rank": "33",
        "paperId": "ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5",
        "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models",
        "reason": "Clarifies terminology by contrasting preference-based “self-play” with environment self-play used here."
      },
      {
        "rank": "34",
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
        "reason": "Referenced to relate Chen et al. (2024) to preference learning approaches unlike this work’s setting."
      },
      {
        "rank": "35",
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback",
        "reason": "Context for the broader wave of methods that improve LMs (contrasted with this work’s self-play finetuning)."
      },
      {
        "rank": "36",
        "paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654",
        "title": "Constitutional AI: Harmlessness from AI Feedback",
        "reason": "Additional context on AI-feedback training methods; background to “autonomous improvement” discussion."
      },
      {
        "rank": "37",
        "paperId": "0671fd553dd670a4e820553a974bc48040ba0819",
        "title": "Reflexion: language agents with verbal reinforcement learning",
        "reason": "Related technique (natural language self-critiques) suggested as promising to combine with self-play in future work."
      },
      {
        "rank": "38",
        "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
        "reason": "General inference-time improvement cited in the “wave of recent work”; peripheral to core method."
      },
      {
        "rank": "39",
        "paperId": "5278a8eb2ba2429d4029745caf4e661080073c81",
        "title": "Generative Agents: Interactive Simulacra of Human Behavior",
        "reason": "Supports the idea that pretrained LMs may simulate diverse personas, relating self-play to population play."
      },
      {
        "rank": "40",
        "paperId": "8c7ed130efb8dd61213784ec9e88f7681944a40a",
        "title": "AI-generated characters for supporting personalized learning and well-being",
        "reason": "Reinforces the “population/persona” perspective on pretrained LMs relevant to interpreting self-play."
      }
    ]
  },
  {
    "paperId": "c6fa25f723a1a842b8452bc04d154e44186d05c1",
    "paperLink": "https://www.semanticscholar.org/paper/c6fa25f723a1a842b8452bc04d154e44186d05c1",
    "paperTitle": "Efficacy of Language Model Self-Play in Non-Zero-Sum Games",
    "chunk": "6",
    "candidates": [
      {
        "rank": "41",
        "paperId": "cf41ae462687f81ce95b27113c6a4f9c2751de42",
        "title": "Vision-Language Models as Success Detectors",
        "reason": "Background for future extension to learned reward models beyond game settings."
      },
      {
        "rank": "42",
        "paperId": "1d2f541bc72544c1f38f39938550029a76cc6db6",
        "title": "Policy Learning with a Language Bottleneck",
        "reason": "Tangential alternative approach (language bottleneck) to improve agents; mentioned as another possible direction."
      },
      {
        "rank": "43",
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning",
        "reason": "Historical reference (REINFORCE) used to describe prior optimization in DoND; minimal direct impact on this paper’s method."
      }
    ]
  }
]