[
  {
    "paperId": "aa933e27c470eeecbe7bbec5debdd8c5d2faa4be",
    "paperLink": "https://www.semanticscholar.org/paper/aa933e27c470eeecbe7bbec5debdd8c5d2faa4be",
    "paperTitle": "Why Does Zero-Shot Cross-Lingual Generation Fail? An Explanation and a Solution",
    "candidates": [
      {
        "rank": "1",
        "paperId": "ffd8f81ed69ddfef6cddc3e8d0eae78f7b13435a",
        "title": "Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation",
        "reason": "Core baseline and problem framing: repeatedly cited for accidental translation evaluation, English-as-source setup, and the parameter-efficient regularization idea the paper compares against and seeks to improve upon."
      },
      {
        "rank": "2",
        "paperId": "fd708dc43c0ed70ed03b2818a3f50fedda6d7f6e",
        "title": "Enhancing Cross-lingual Transfer by Manifold Mixup",
        "reason": "Central to the paper’s thesis and analysis: provides the XLRS–performance perspective; the paper follows its procedure for correlating cross-lingual representation similarity with transfer performance and then challenges the prevailing “more invariance is better” view for generation."
      },
      {
        "rank": "3",
        "paperId": "d592007d1c106fe1217604eb35664c7a5f07cb32",
        "title": "Multilingual Alignment of Contextual Word Representations",
        "reason": "Foundational evidence for language-invariant representations aiding transfer; directly cited as the dominant view that the paper explains is harmful for generation and seeks to regularize against."
      },
      {
        "rank": "4",
        "paperId": "ba4a34680e09e77984624c95f5245d91b54373f6",
        "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization",
        "reason": "Key background connecting cross-lingual transfer to increased XLRS and showing prior focus on classification; the paper builds on and contrasts this behavior in generation."
      },
      {
        "rank": "5",
        "paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a",
        "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
        "reason": "Primary model used throughout experiments; also a source citing wrong-language generation issues that motivate the paper’s solution."
      },
      {
        "rank": "6",
        "paperId": "f00f2d4b8ddd55aa2cc202f44053e5f97a254175",
        "title": "WikiLingua: A New Benchmark Dataset for Multilingual Abstractive Summarization",
        "reason": "Main abstractive generation benchmark in the paper’s experiments and analyses (tables and model selection comparisons)."
      },
      {
        "rank": "7",
        "paperId": "ddde271db4611bf7cd821f8b89ecf701c2e34d5b",
        "title": "MTG: A Benchmark Suite for Multilingual Text Generation",
        "reason": "Supplies the story completion and title generation tasks central to evaluating the proposed method and the accidental translation reduction."
      },
      {
        "rank": "8",
        "paperId": "5e8180e2ceddaab161e9be55bd81d8f911967302",
        "title": "Model Selection for Cross-lingual Transfer",
        "reason": "Prior work on cross-lingual model selection the paper positions against; informs the paper’s proposal of using XLRS-based checkpoint selection without target dev sets."
      },
      {
        "rank": "9",
        "paperId": "da6fa3c0c4145e1d3b5e98fc2494f993c237c782",
        "title": "Zero-shot Cross-lingual Transfer is Under-specified Optimization",
        "reason": "Supports the paper’s explanation angle: fine-tuning can land in parameter regions that work for source but not all targets; under-specification motivates regularizing training dynamics."
      },
      {
        "rank": "10",
        "paperId": "a225aab16b852dd2cc12cf72749ff9609656389b",
        "title": "Improving Multilingual Neural Machine Translation with Auxiliary Source Languages",
        "reason": "Direct motivation for the paper’s auxiliary-source regularization of XLRS during fine-tuning."
      },
      {
        "rank": "11",
        "paperId": "04a7021fe6be6bddcfae476493fcc7571e7c613c",
        "title": "PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification",
        "reason": "Core classification task used to contrast with generation: plots and XLRS dynamics for tasks with high label overlap."
      },
      {
        "rank": "12",
        "paperId": "57133ef4c4de4d54a57686b8a914b06e4ff4aab5",
        "title": "Universal Dependencies 2.1",
        "reason": "Supplies the UDPOS tagging data used to study classification vs. generation differences in XLRS and transfer."
      },
      {
        "rank": "13",
        "paperId": "83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6",
        "title": "TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages",
        "reason": "Extractive generation task used to evaluate the proposed regularization and model selection behavior."
      },
      {
        "rank": "14",
        "paperId": "f35dbee22c1572d149b7c1e20d69672cae931451",
        "title": "Transforming Sequence Tagging Into A Seq2Seq Task",
        "reason": "Practical modeling step the paper adopts to cast sequence labeling into seq2seq for unified analysis across tasks."
      },
      {
        "rank": "15",
        "paperId": "8ae9a17c87a4518b513e860683a0ef7824be994d",
        "title": "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference",
        "reason": "Guides the paper’s casting of entailment-style classification as cloze prompting for consistent supervision."
      },
      {
        "rank": "16",
        "paperId": "415f924c5c79e300891881af367e4d77602f9f39",
        "title": "Cross-Lingual Transfer with Class-Weighted Language-Invariant Representations",
        "reason": "Related approach on managing language-invariant representations; supports the paper’s argument that naïve invariance can be harmful depending on task."
      },
      {
        "rank": "17",
        "paperId": "c0a54963b0689fa7d76fda1063b65003c769d9b7",
        "title": "Language-agnostic Representation from Multilingual Sentence Encoders for Cross-lingual Similarity Estimation",
        "reason": "Representative work underlying the “similar cross-lingual representations help transfer” consensus the paper challenges for generation."
      },
      {
        "rank": "18",
        "paperId": "cfe8ec7a183ed548db1a862e38908343cefb94c7",
        "title": "Emerging Cross-lingual Structure in Pretrained Language Models",
        "reason": "Empirical support that multilingual LMs learn cross-lingual structure/invariance; background for the XLRS-based explanation."
      },
      {
        "rank": "19",
        "paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6",
        "title": "Unsupervised Cross-lingual Representation Learning at Scale",
        "reason": "Establishes strong multilingual encoders (XLM-R) and the invariance-transfer link cited as the prevailing belief the paper scrutinizes for generation."
      },
      {
        "rank": "20",
        "paperId": "495da6f19baa09c6db3697d839e10432cdc25934",
        "title": "Multilingual Denoising Pre-training for Neural Machine Translation",
        "reason": "Architectural background (mBART) and cross-lingual pretraining context relevant to model choices and invariance discussions."
      },
      {
        "rank": "21",
        "paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation",
        "reason": "General encoder–decoder pretraining background underpinning mBART/mT5 and the paper’s seq2seq setup."
      },
      {
        "rank": "22",
        "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "reason": "T5 background underpinning mT5; informs the paper’s model selection and training framing for text-to-text tasks."
      },
      {
        "rank": "23",
        "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "reason": "General LM pretraining background that situates multilingual variants and the cross-lingual transfer literature the paper builds upon."
      },
      {
        "rank": "24",
        "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
        "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "reason": "Background on strong monolingual pretraining that underlies multilingual variants referenced in the paper."
      },
      {
        "rank": "25",
        "paperId": "06431546c21d7c2528aaa170c2e1078e0a82d12e",
        "title": "Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer",
        "reason": "Context for source-language choice; the paper acknowledges task-dependent benefits of non-English sources while using English for consistency."
      },
      {
        "rank": "26",
        "paperId": "248824ec5d9b4ddf0c36cdc51b6b57af6e881328",
        "title": "Choosing Transfer Languages for Cross-Lingual Learning",
        "reason": "Source-language selection literature the paper references when motivating generalization beyond English-only fine-tuning."
      },
      {
        "rank": "27",
        "paperId": "142d60c73140c82ce382525fcac6358999d71627",
        "title": "Evaluating the Values of Sources in Transfer Learning",
        "reason": "Prior methods (ranking/Shapley) for choosing sources; background for the paper’s simpler, representation-based regularization/selection."
      },
      {
        "rank": "28",
        "paperId": "56d753c5f161322470721e3441d0910f568a2031",
        "title": "Zero-Shot Cross-Lingual Transfer with Meta Learning",
        "reason": "Another approach to source/transfer selection; context for the paper’s non-meta-learning alternative using XLRS dynamics."
      },
      {
        "rank": "29",
        "paperId": "6aca5e5d6919dcc6986813b16a910b8db41b59f4",
        "title": "Don’t Stop Fine-Tuning: On Training Regimes for Few-Shot Cross-Lingual Transfer with Multilingual Language Models",
        "reason": "Related few-shot cross-lingual training insights, including benefits of multiple languages that connect to the paper’s auxiliary-source idea."
      },
      {
        "rank": "30",
        "paperId": "7317dccaf8023b2719a2d0fe787a31b20a3232e1",
        "title": "A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters",
        "reason": "Highlights true zero-shot model selection difficulty; supports the paper’s emphasis on dev-free checkpoint selection."
      },
      {
        "rank": "31",
        "paperId": "811a5c79d8c0f6f5b57697e7be0e84b5f9a94ce8",
        "title": "Por Qué Não Utiliser Alla Språk? Mixed Training with Gradient Optimization in Few-Shot Cross-Lingual Transfer",
        "reason": "Contemporary evidence that multi-language fine-tuning helps; aligns with the paper’s auxiliary-source regularization rationale."
      },
      {
        "rank": "32",
        "paperId": "e514a03a17bd6dfce12d3c450a93c8175e8de3eb",
        "title": "A Call for More Rigor in Unsupervised Cross-lingual Learning",
        "reason": "Warns about translationese and evaluation pitfalls; supports the paper’s dataset considerations and practical setting assumptions."
      },
      {
        "rank": "33",
        "paperId": "085b360d3c08aaf997f45a78e27f2629f5625205",
        "title": "Translation Artifacts in Cross-lingual Transfer Learning",
        "reason": "Further cautions about translation artifacts; informs dataset selection/interpretation in the paper."
      },
      {
        "rank": "34",
        "paperId": "3b2538f84812f434c740115c185be3e5e216c526",
        "title": "Cross-Lingual Ability of Multilingual BERT: An Empirical Study",
        "reason": "Background on factors (word order) affecting transfer; contextualizes when invariance helps vs. hurts."
      },
      {
        "rank": "35",
        "paperId": "c3a662b864673d8cc7469051419ab8819926d4b0",
        "title": "Identifying Elements Essential for BERT’s Multilinguality",
        "reason": "Script/word-order debates providing nuance for invariance and transfer—context for the paper’s explanation."
      },
      {
        "rank": "36",
        "paperId": "6bf7c93ed5a3aca5ef139308c6797615461daa39",
        "title": "Match the Script",
        "reason": "Adapt if Multilingual: Analyzing the Effect of Multilingual Pretraining on Cross-lingual Transferability, Additional evidence on script’s role; supports the broader transferability backdrop the paper contrasts with generation."
      },
      {
        "rank": "37",
        "paperId": "fd6bc84144c2d77068bf3f077cb509d539f5f8e2",
        "title": "On the Cross-lingual Transferability of Monolingual Representations",
        "reason": "Complementary results on what enables transfer (e.g., word order), grounding the paper’s claims about representation properties."
      },
      {
        "rank": "38",
        "paperId": "1a7591aefa9b4129e37cda2ad97d1d1cba460b38",
        "title": "On Difficulties of Cross-Lingual Transfer with Order Differences: A Case Study on Dependency Parsing",
        "reason": "Reinforces the impact of syntactic differences; contextual backdrop for when XLRS may or may not be beneficial."
      },
      {
        "rank": "39",
        "paperId": "1c3112ef8a346b9817382ed34a8c146c53d5bcf5",
        "title": "XNLI: Evaluating Cross-lingual Sentence Representations",
        "reason": "Widely used classification benchmark framing cross-lingual evaluation; situates the paper’s classification vs. generation contrast."
      },
      {
        "rank": "40",
        "paperId": "5b1516c87818084dc5d195cc274e1ee8923210d2",
        "title": "Neural Cross-Lingual Named Entity Recognition with Minimal Resources",
        "reason": "General cross-lingual application background; helps motivate broader transfer claims contrasted with generation findings."
      },
      {
        "rank": "41",
        "paperId": "48e8e8085907192d501eb2bcc582035e90431a2f",
        "title": "Multi-Task Cross-Lingual Sequence Tagging from Scratch",
        "reason": "Additional background on cross-lingual tagging; contextualizes the classification task setup."
      },
      {
        "rank": "42",
        "paperId": "143cd4b4717651caf276c7256502dc491454e197",
        "title": "The Curious Case of Hallucinations in Neural Machine Translation",
        "reason": "Part of the pathology background (hallucinations) the paper cites alongside wrong-language errors."
      },
      {
        "rank": "43",
        "paperId": "6151ee4af6a3fe78f2df7c605598cd9e02b23c5b",
        "title": "Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation",
        "reason": "Cited to characterize generation pathologies (repetition) that co-occur with wrong-language outputs."
      },
      {
        "rank": "44",
        "paperId": "4789355a00650afa29682d331d7d9a49a301b43b",
        "title": "Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation",
        "reason": "Background on zero-shot generation quality issues (incl. wrong-language output) that the paper aims to reduce."
      },
      {
        "rank": "45",
        "paperId": "af6fce169699592beaa379a6532dd192fad3f13f",
        "title": "Alternative Input Signals Ease Transfer in Multilingual Machine Translation",
        "reason": "Alternative techniques to improve generation cited as related work; contrasts with the paper’s representation-regularization approach."
      },
      {
        "rank": "46",
        "paperId": "b3e3a2ec4e411e07a31972745e2eea537b9eb20c",
        "title": "Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations",
        "reason": "Another alternative path to better zero-shot generation; contextual related work."
      },
      {
        "rank": "47",
        "paperId": "38d3657ee15f2612330eb5e036bbc38d9137f75a",
        "title": "ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation",
        "reason": "Related generation-focused transfer work; helps situate the paper within cross-lingual generation literature."
      },
      {
        "rank": "48",
        "paperId": "23ae98d341d9e587f37ef84096d6a0186bdfd284",
        "title": "Meta-X_{NLG}: A Meta-Learning Approach Based on Language Clustering for Zero-Shot Cross-Lingual Transfer and Generation",
        "reason": "Related to generation-focused zero-shot transfer; background for the paper’s simpler regularization approach."
      },
      {
        "rank": "49",
        "paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4",
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
        "reason": "Parameter-efficient fine-tuning the paper cites via Vu et al. as an alternative to control language; contrasts with the proposed method."
      },
      {
        "rank": "50",
        "paperId": "53d8b356551a2361020a948f64454a6d599af69f",
        "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "reason": "As above; provides a competing mechanism to steer generation language without full fine-tuning."
      },
      {
        "rank": "51",
        "paperId": "209f9bde2dee7cf1677801586562ffe56d435d38",
        "title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts",
        "reason": "Another parameter-efficient approach the paper references as related but distinct from its representation regularization."
      },
      {
        "rank": "52",
        "paperId": "149c123e7ffc3658a3d51c0dc5637764257318f3",
        "title": "Multilingual and Zero-Shot is Closing in on Monolingual Web Register Classification",
        "reason": "Background noting generation incoherence/wrong-language issues; peripheral evidence for the problem statement."
      },
      {
        "rank": "53",
        "paperId": "2fa3f7ce620a1c7155daef6620dd6bb0e01934f3",
        "title": "Beto",
        "reason": "Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT, General cross-lingual transfer background with mBERT; helps frame zero-shot capabilities contrasted with generation failures."
      },
      {
        "rank": "54",
        "paperId": "4f4a409f701f7552d45c46a5b0fea69dca6f8e84",
        "title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
        "reason": "Only cited incidentally in a general list of multilingual applications; minimal direct influence on the paper’s explanation, method, or experiments."
      }
    ]
  }
]